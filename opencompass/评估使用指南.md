# OpenCompass 大模型评估使用指南

本指南将帮助你在 `/data/hxf/bigmodel/opencompass` 目录下使用 OpenCompass 进行大模型评估。

## 环境准备

首先确保已激活正确的 conda 环境：

```bash
conda activate vllm_google
cd /data/hxf/bigmodel/opencompass
```

## 快速开始

### 方法1: 使用 vLLM API 进行评估（推荐）

如果你的模型已经通过 vLLM 部署为 API 服务，可以使用以下方式评估：

```bash
# 1. 编辑配置文件 eval_vllm_api.py，修改 API 地址和模型配置
# 2. 运行评估
python run.py eval_vllm_api.py --debug
```

**配置说明：**

编辑 `eval_vllm_api.py` 文件，主要需要修改以下部分：

1. **vLLM API 地址**：
   ```python
   VLLM_API_BASE = 'http://192.168.1.212:8056/v1'  # 修改为你的 API 地址
   ```

2. **模型标识名称**：
   ```python
   MODEL_ABBR = 'your-vllm-model'  # 修改为你的模型名称
   ```

3. **对话模板 (meta_template)**：
   - 如果使用标准 OpenAI ChatML 格式，使用：
     ```python
     api_meta_template = dict(
         round=[
             dict(role='HUMAN', api_role='user'),
             dict(role='BOT', api_role='assistant', generate=True),
         ],
     )
     ```
   - 如果是自定义格式，参考 `eval_chinese_vllm.py` 中的配置方式

4. **数据集选择**：
   ```python
   # 可以选择不同的数据集
   datasets = [*ceval_datasets]  # 中文评测数据集
   # datasets = gsm8k_datasets + math_datasets  # 英文数学推理数据集
   ```

**参考配置**：`/data/hxf/bigmodel/opencompass/opencompass/configs/eval_chinese_vllm.py`

### 方法2: 使用命令行进行快速评估

#### 评估 HuggingFace 模型（推荐）

```bash
# 评估对话模型 (Chat Model)
python run.py \
    --datasets demo_gsm8k_chat_gen \
    --hf-type chat \
    --hf-path YOUR_MODEL_PATH \
    --max-out-len 1024 \
    --debug

# 评估基座模型 (Base Model)
python run.py \
    --datasets demo_gsm8k_base_gen \
    --hf-type base \
    --hf-path YOUR_MODEL_PATH \
    --max-out-len 1024 \
    --debug
```

#### 使用预定义的模型配置

```bash
# 列出所有可用模型配置
python tools/list_configs.py | grep model

# 列出所有可用数据集配置
python tools/list_configs.py | grep dataset

# 使用预定义配置进行评估
python run.py \
    --models hf_qwen2_1_5b_instruct \
    --datasets demo_gsm8k_chat_gen demo_math_chat_gen \
    --debug
```

### 方法3: 使用配置文件进行评估（HuggingFace 模型）

编辑 `eval_example.py` 文件，修改要评估的模型和数据集，然后运行：

```bash
python run.py eval_example.py --debug
```

**注意**：`eval_example.py` 是为 HuggingFace 模型设计的。如果使用 vLLM API，请使用方法1中的 `eval_vllm_api.py`。

## 常用评估命令示例

### 1. 基础评估（单个数据集）

```bash
# 评估 GSM8K 数学推理能力
python run.py \
    --datasets demo_gsm8k_chat_gen \
    --hf-type chat \
    --hf-path YOUR_MODEL_PATH \
    --debug
```

### 2. 多数据集评估

```bash
# 同时评估多个数据集
python run.py \
    --datasets demo_gsm8k_chat_gen demo_math_chat_gen \
    --hf-type chat \
    --hf-path YOUR_MODEL_PATH \
    --debug
```

### 3. 使用完整数据集集合评估

```bash
# 使用 OC15 数据集集合（包含15个常用评测数据集）
python run.py \
    --datasets chat_OC15 \
    --hf-type chat \
    --hf-path YOUR_MODEL_PATH \
    --debug
```

### 4. 多GPU并行评估

```bash
# 使用2个GPU进行模型并行
CUDA_VISIBLE_DEVICES=0,1 python run.py \
    --datasets demo_gsm8k_chat_gen \
    --hf-type chat \
    --hf-path YOUR_MODEL_PATH \
    --hf-num-gpus 2 \
    --debug

# 使用数据并行（多个worker）
CUDA_VISIBLE_DEVICES=0,1 python run.py \
    --datasets demo_gsm8k_chat_gen \
    --hf-type chat \
    --hf-path YOUR_MODEL_PATH \
    --max-num-worker 2 \
    --debug
```

### 5. 使用推理加速后端（可选）

```bash
# 使用 LMDeploy 加速
python run.py \
    --datasets demo_gsm8k_chat_gen \
    --hf-type chat \
    --hf-path YOUR_MODEL_PATH \
    -a lmdeploy \
    --debug

# 使用 vLLM 加速
python run.py \
    --datasets demo_gsm8k_chat_gen \
    --hf-type chat \
    --hf-path YOUR_MODEL_PATH \
    -a vllm \
    --debug
```

### 6. 生产环境运行（非debug模式）

```bash
# 移除 --debug 参数，任务将在后台并行执行
python run.py \
    --datasets demo_gsm8k_chat_gen \
    --hf-type chat \
    --hf-path YOUR_MODEL_PATH \
    -w outputs/my_evaluation
```

## 参数说明

### 主要参数

- `--datasets`: 要评估的数据集列表，多个数据集用空格分隔
- `--models`: 使用预定义模型配置的模型名称
- `--hf-type`: HuggingFace 模型类型，可选 `chat` 或 `base`
- `--hf-path`: HuggingFace 模型路径（HuggingFace Hub ID 或本地路径）
- `--max-out-len`: 生成的最大token数（默认2048）
- `--batch-size`: 批量大小（默认8）
- `--hf-num-gpus`: 模型并行所需的GPU数量
- `--max-num-worker`: 数据并行的worker数量
- `-w` / `--work-dir`: 输出目录（默认 outputs/default）
- `--debug`: 调试模式，顺序执行并实时显示输出
- `-a` / `--accelerator`: 推理加速后端（lmdeploy/vllm）

### 高级参数

```bash
# 自定义生成参数
--generation-kwargs do_sample=True temperature=0.7 top_p=0.9

# 自定义停用词
--stop-words '<|im_end|>' '<|im_start|>'

# 指定tokenizer路径
--tokenizer-path YOUR_TOKENIZER_PATH

# 重用已有结果
-r latest  # 重用最新结果
-r TIMESTAMP  # 重用指定时间戳的结果
```

## 查看评估结果

评估完成后，结果会保存在 `outputs/` 目录下：

```
outputs/
└── your_work_dir/
    └── TIMESTAMP/
        ├── configs/      # 配置文件
        ├── logs/         # 日志文件
        │   ├── infer/    # 推理日志
        │   └── eval/     # 评估日志
        ├── predictions/  # 模型预测结果
        ├── results/      # 评估结果
        └── summary/      # 汇总结果（CSV和TXT格式）
```

查看汇总结果：

```bash
cat outputs/your_work_dir/TIMESTAMP/summary/*.csv
cat outputs/your_work_dir/TIMESTAMP/summary/*.txt
```

## 常用数据集列表

- **数学推理**: `demo_gsm8k_chat_gen`, `demo_math_chat_gen`
- **常识推理**: `hellaswag_gen`, `piqa_gen`
- **代码能力**: `humaneval_gen`, `mbpp_gen`
- **知识问答**: `mmlu_gen`, `ceval_gen`, `cmmlu_gen`
- **综合评估**: `chat_OC15` (包含15个常用数据集)

查看所有可用数据集：

```bash
python tools/list_configs.py | grep dataset
```

## 评估自定义模型

### 1. vLLM API 模型（推荐）

参考 `eval_vllm_api.py` 配置文件，这是评估 vLLM 部署 API 的标准方式。

**关键配置项：**

```python
models = [
    dict(
        type=OpenAI,  # 使用 OpenAI 包装器
        abbr='your-model-name',
        path='model-path',  # 模型标识
        key='EMPTY',  # vLLM API 通常不需要 key
        openai_api_base='http://your-api-server:port/v1',  # API 地址
        meta_template=api_meta_template,  # 对话模板（重要！）
        query_per_second=10,  # QPS 限制
        max_out_len=2048,
        batch_size=8,
        mode='chat',
    )
]
```

**meta_template 配置说明：**

`meta_template` 必须与你的 vLLM 部署时使用的对话格式匹配：

- **标准格式**（大多数情况）：
  ```python
  api_meta_template = dict(
      round=[
          dict(role='HUMAN', api_role='user'),
          dict(role='BOT', api_role='assistant', generate=True),
      ],
  )
  ```

- **自定义格式**（参考 eval_chinese_vllm.py）：
  ```python
  meta_template = dict(
      round=[
          dict(role='HUMAN', begin='[[HUMAN]]\n', end='\n'),
          dict(role='BOT', begin='[[BOT]]\n', end='\n', generate=True)
      ],
  )
  ```

### 2. 本地 HuggingFace 模型路径

```bash
python run.py \
    --datasets demo_gsm8k_chat_gen \
    --hf-type chat \
    --hf-path /path/to/your/local/model \
    --debug
```

### 3. HuggingFace Hub 模型

```bash
python run.py \
    --datasets demo_gsm8k_chat_gen \
    --hf-type chat \
    --hf-path Qwen/Qwen2-1.5B-Instruct \
    --debug
```

## 常见问题

### 1. 内存不足

- 减小 `--batch-size`
- 使用模型并行 `--hf-num-gpus 2`
- 使用更小的数据集进行测试

### 2. 网络问题

如果无法从 HuggingFace 下载模型，可以：
- 提前下载模型到本地
- 使用本地模型路径

### 3. 查看详细日志

在非debug模式下，查看日志：

```bash
# 查看推理日志
tail -f outputs/your_work_dir/TIMESTAMP/logs/infer/*.log

# 查看评估日志
tail -f outputs/your_work_dir/TIMESTAMP/logs/eval/*.log
```

## 更多资源

- [OpenCompass 官方文档](https://opencompass.readthedocs.io/zh_CN/latest/index.html)
- [快速开始指南](https://opencompass.readthedocs.io/zh_CN/latest/get_started/quick_start.html)
- [模型配置指南](https://opencompass.readthedocs.io/zh_CN/latest/user_guides/models.html)
- [数据集配置指南](https://opencompass.readthedocs.io/zh_CN/latest/user_guides/datasets.html)

## 示例配置文件

参考 `eval_example.py` 文件，了解如何编写评估配置文件。


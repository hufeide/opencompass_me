DATASET_SOURCE=ModelScope python run.py eval_ceval2.py --debug
对cevel进行评测


Model: 
ceval-computer_network: {'accuracy': 63.1578947368421}
ceval-operating_system: {'accuracy': 89.47368421052632}
ceval-computer_architecture: {'accuracy': 80.95238095238095}
ceval-college_programming: {'accuracy': 75.67567567567568}
ceval-college_physics: {'accuracy': 42.10526315789473}
ceval-college_chemistry: {'accuracy': 45.83333333333333}
ceval-advanced_mathematics: {'accuracy': 21.052631578947366}
ceval-probability_and_statistics: {'accuracy': 0.0}
ceval-discrete_mathematics: {'accuracy': 18.75}
ceval-electrical_engineer: {'accuracy': 62.16216216216216}
ceval-metrology_engineer: {'accuracy': 83.33333333333334}
ceval-high_school_mathematics: {'accuracy': 22.22222222222222}
ceval-high_school_physics: {'accuracy': 78.94736842105263}
ceval-high_school_chemistry: {'accuracy': 57.89473684210527}
ceval-high_school_biology: {'accuracy': 84.21052631578947}
ceval-middle_school_mathematics: {'accuracy': 52.63157894736842}
ceval-middle_school_biology: {'accuracy': 85.71428571428571}
ceval-middle_school_physics: {'accuracy': 94.73684210526315}
ceval-middle_school_chemistry: {'accuracy': 100.0}
ceval-veterinary_medicine: {'accuracy': 86.95652173913044}
ceval-college_economics: {'accuracy': 72.72727272727273}
ceval-business_administration: {'accuracy': 81.81818181818183}
ceval-marxism: {'accuracy': 94.73684210526315}
ceval-mao_zedong_thought: {'accuracy': 100.0}
ceval-education_science: {'accuracy': 96.55172413793103}
ceval-teacher_qualification: {'accuracy': 95.45454545454545}
ceval-high_school_politics: {'accuracy': 94.73684210526315}
ceval-high_school_geography: {'accuracy': 100.0}
ceval-middle_school_politics: {'accuracy': 100.0}
ceval-middle_school_geography: {'accuracy': 91.66666666666666}
ceval-modern_chinese_history: {'accuracy': 86.95652173913044}
ceval-ideological_and_moral_cultivation: {'accuracy': 94.73684210526315}
ceval-logic: {'accuracy': 68.18181818181817}
ceval-law: {'accuracy': 87.5}
ceval-chinese_language_and_literature: {'accuracy': 86.95652173913044}
ceval-art_studies: {'accuracy': 90.9090909090909}
ceval-professional_tour_guide: {'accuracy': 96.55172413793103}
ceval-legal_professional: {'accuracy': 78.26086956521739}
ceval-high_school_chinese: {'accuracy': 73.68421052631578}
ceval-high_school_history: {'accuracy': 95.0}
ceval-middle_school_history: {'accuracy': 100.0}
ceval-civil_servant: {'accuracy': 76.59574468085107}
ceval-sports_science: {'accuracy': 84.21052631578947}
ceval-plant_protection: {'accuracy': 100.0}
ceval-basic_medicine: {'accuracy': 84.21052631578947}
ceval-clinical_medicine: {'accuracy': 77.27272727272727}
ceval-urban_and_rural_planner: {'accuracy': 91.30434782608695}
ceval-accountant: {'accuracy': 85.71428571428571}
ceval-fire_engineer: {'accuracy': 87.09677419354838}
ceval-environmental_impact_assessment_engineer: {'accuracy': 70.96774193548387}
ceval-tax_accountant: {'accuracy': 89.79591836734694}
ceval-physician: {'accuracy': 89.79591836734694}
ceval-stem: {'accuracy': 62.29052207241566, 'naive_average': 62.29052207241566}
ceval-social-science: {'accuracy': 92.7692075015124, 'naive_average': 92.7692075015124}
ceval-humanities: {'accuracy': 87.15796353671793, 'naive_average': 87.15796353671793}
ceval-other: {'accuracy': 85.17859190811419, 'naive_average': 85.17859190811419}
ceval-hard: {'accuracy': 35.85069444444444, 'naive_average': 35.85069444444444}
ceval: {'accuracy': 78.25393512224213, 'naive_average': 78.25393512224213}
